/**
 * æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå†…å­˜æ³„æ¼æ£€æµ‹
 * 
 * åŠŸèƒ½ï¼š
 * 1. æ€§èƒ½åŸºå‡†æµ‹è¯•
 * 2. å†…å­˜æ³„æ¼æ£€æµ‹
 * 3. å‹åŠ›æµ‹è¯•
 * 4. æ€§èƒ½å¯¹æ¯”åˆ†æ
 */

import { HttpClientImpl } from '../client'
import { createAdapter } from '../adapters'
import { MemoryCacheStorage } from '../utils/cache'
import { BatchOptimizer } from '../utils/batch-optimizer'
import type { HttpClientConfig } from '../types'

/**
 * æ€§èƒ½æµ‹è¯•é…ç½®
 */
interface BenchmarkConfig {
  /** æµ‹è¯•åç§° */
  name: string
  /** è¯·æ±‚æ•°é‡ */
  requests: number
  /** å¹¶å‘æ•°é‡ */
  concurrency: number
  /** æ˜¯å¦å¯ç”¨ç¼“å­˜ */
  cache?: boolean
  /** æ˜¯å¦å¯ç”¨æ‰¹å¤„ç† */
  batching?: boolean
  /** æ˜¯å¦å¯ç”¨ä¼˜å…ˆçº§é˜Ÿåˆ— */
  priorityQueue?: boolean
}

/**
 * æ€§èƒ½æµ‹è¯•ç»“æœ
 */
interface BenchmarkResult {
  name: string
  totalRequests: number
  totalTime: number
  averageTime: number
  requestsPerSecond: number
  memoryUsage: MemoryUsage
  errors: number
  p50: number
  p95: number
  p99: number
}

/**
 * å†…å­˜ä½¿ç”¨æƒ…å†µ
 */
interface MemoryUsage {
  initial: number
  peak: number
  final: number
  leaked: number
}

/**
 * æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»
 */
export class PerformanceBenchmark {
  private results: BenchmarkResult[] = []
  
  /**
   * è¿è¡ŒåŸºå‡†æµ‹è¯•
   */
  async runBenchmark(config: BenchmarkConfig): Promise<BenchmarkResult> {
    console.log(`ğŸš€ Running benchmark: ${config.name}`)
    
    // è®°å½•åˆå§‹å†…å­˜
    const initialMemory = this.getMemoryUsage()
    let peakMemory = initialMemory
    
    // åˆ›å»ºå®¢æˆ·ç«¯
    const clientConfig: HttpClientConfig = {
      baseURL: 'https://api.example.com',
      timeout: 30000,
      cache: {
        enabled: config.cache ?? false,
        storage: new MemoryCacheStorage(1000),
      },
      concurrency: {
        maxConcurrent: config.concurrency,
      },
      priorityQueue: {
        maxConcurrent: config.priorityQueue ? 6 : undefined,
      },
    }
    
    const adapter = createAdapter()
    const client = new HttpClientImpl(clientConfig, adapter)
    
    // æ‰¹å¤„ç†ä¼˜åŒ–å™¨
    let batchOptimizer: BatchOptimizer | undefined
    if (config.batching) {
      batchOptimizer = new BatchOptimizer({
        enabled: true,
        maxBatchSize: 10,
        batchWindow: 50,
      })
    }
    
    // æ€§èƒ½æ•°æ®æ”¶é›†
    const durations: number[] = []
    let errors = 0
    
    // å¼€å§‹æµ‹è¯•
    const startTime = Date.now()
    
    // åˆ›å»ºè¯·æ±‚ä»»åŠ¡
    const tasks: Promise<void>[] = []
    for (let i = 0; i < config.requests; i++) {
      const task = this.executeRequest(client, batchOptimizer, i)
        .then(duration => {
          durations.push(duration)
          
          // æ›´æ–°å³°å€¼å†…å­˜
          const currentMemory = this.getMemoryUsage()
          if (currentMemory > peakMemory) {
            peakMemory = currentMemory
          }
        })
        .catch(() => {
          errors++
        })
      
      tasks.push(task)
      
      // æ§åˆ¶å¹¶å‘
      if (tasks.length >= config.concurrency) {
        await Promise.race(tasks)
        tasks.splice(tasks.findIndex(t => t), 1)
      }
    }
    
    // ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
    await Promise.all(tasks)
    
    const totalTime = Date.now() - startTime
    
    // è®°å½•æœ€ç»ˆå†…å­˜
    const finalMemory = this.getMemoryUsage()
    
    // æ¸…ç†èµ„æº
    client.destroy()
    if (batchOptimizer) {
      batchOptimizer.destroy()
    }
    
    // å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    this.forceGC()
    await this.sleep(100)
    
    // æ£€æŸ¥å†…å­˜æ³„æ¼
    const afterGCMemory = this.getMemoryUsage()
    const leaked = afterGCMemory - initialMemory
    
    // è®¡ç®—ç»Ÿè®¡æ•°æ®
    durations.sort((a, b) => a - b)
    const result: BenchmarkResult = {
      name: config.name,
      totalRequests: config.requests,
      totalTime,
      averageTime: durations.reduce((a, b) => a + b, 0) / durations.length,
      requestsPerSecond: (config.requests / totalTime) * 1000,
      memoryUsage: {
        initial: initialMemory,
        peak: peakMemory,
        final: finalMemory,
        leaked: Math.max(0, leaked),
      },
      errors,
      p50: this.getPercentile(durations, 50),
      p95: this.getPercentile(durations, 95),
      p99: this.getPercentile(durations, 99),
    }
    
    this.results.push(result)
    this.printResult(result)
    
    return result
  }
  
  /**
   * æ‰§è¡Œå•ä¸ªè¯·æ±‚
   */
  private async executeRequest(
    client: HttpClientImpl,
    batchOptimizer: BatchOptimizer | undefined,
    index: number
  ): Promise<number> {
    const startTime = Date.now()
    
    if (batchOptimizer) {
      await batchOptimizer.addRequest(
        {
          url: `/api/users/${index % 100}`,
          method: 'GET',
        },
        (config) => client.request(config)
      )
    } else {
      await client.get(`/api/users/${index % 100}`)
    }
    
    return Date.now() - startTime
  }
  
  /**
   * è¿è¡Œå†…å­˜æ³„æ¼æµ‹è¯•
   */
  async runMemoryLeakTest(): Promise<void> {
    console.log('ğŸ” Running memory leak detection...')
    
    const iterations = 10
    const requestsPerIteration = 100
    const memoryHistory: number[] = []
    
    for (let i = 0; i < iterations; i++) {
      // åˆ›å»ºå’Œé”€æ¯å®¢æˆ·ç«¯
      const adapter = createAdapter()
      const client = new HttpClientImpl({}, adapter)
      
      // æ‰§è¡Œè¯·æ±‚
      const promises: Promise<any>[] = []
      for (let j = 0; j < requestsPerIteration; j++) {
        promises.push(
          client.get(`/api/test/${j}`).catch(() => {})
        )
      }
      
      await Promise.all(promises)
      
      // é”€æ¯å®¢æˆ·ç«¯
      client.destroy()
      
      // å¼ºåˆ¶GCå¹¶è®°å½•å†…å­˜
      this.forceGC()
      await this.sleep(100)
      
      const memory = this.getMemoryUsage()
      memoryHistory.push(memory)
      
      console.log(`  Iteration ${i + 1}: ${this.formatMemory(memory)}`)
    }
    
    // åˆ†æå†…å­˜å¢é•¿
    const memoryGrowth = memoryHistory[memoryHistory.length - 1] - memoryHistory[0]
    const averageGrowth = memoryGrowth / iterations
    
    console.log('ğŸ“Š Memory leak analysis:')
    console.log(`  Total growth: ${this.formatMemory(memoryGrowth)}`)
    console.log(`  Average growth per iteration: ${this.formatMemory(averageGrowth)}`)
    
    if (averageGrowth > 1024 * 1024) { // 1MB
      console.log('  âš ï¸ WARNING: Potential memory leak detected!')
    } else {
      console.log('  âœ… No significant memory leak detected')
    }
  }
  
  /**
   * è¿è¡Œå‹åŠ›æµ‹è¯•
   */
  async runStressTest(): Promise<void> {
    console.log('ğŸ’ª Running stress test...')
    
    const configs: BenchmarkConfig[] = [
      {
        name: 'Baseline',
        requests: 1000,
        concurrency: 10,
      },
      {
        name: 'With Cache',
        requests: 1000,
        concurrency: 10,
        cache: true,
      },
      {
        name: 'With Batching',
        requests: 1000,
        concurrency: 10,
        batching: true,
      },
      {
        name: 'With Priority Queue',
        requests: 1000,
        concurrency: 10,
        priorityQueue: true,
      },
      {
        name: 'Full Optimization',
        requests: 1000,
        concurrency: 10,
        cache: true,
        batching: true,
        priorityQueue: true,
      },
      {
        name: 'High Concurrency',
        requests: 1000,
        concurrency: 100,
      },
      {
        name: 'Very High Load',
        requests: 5000,
        concurrency: 50,
        cache: true,
        batching: true,
      },
    ]
    
    for (const config of configs) {
      await this.runBenchmark(config)
      await this.sleep(1000) // å†·å´æ—¶é—´
    }
    
    this.printComparison()
  }
  
  /**
   * æ‰“å°ç»“æœ
   */
  private printResult(result: BenchmarkResult): void {
    console.log(`
ğŸ“ˆ Results for ${result.name}:
  â”œâ”€ Total Requests: ${result.totalRequests}
  â”œâ”€ Total Time: ${result.totalTime}ms
  â”œâ”€ Average Time: ${result.averageTime.toFixed(2)}ms
  â”œâ”€ Requests/Second: ${result.requestsPerSecond.toFixed(2)}
  â”œâ”€ Errors: ${result.errors}
  â”œâ”€ P50: ${result.p50.toFixed(2)}ms
  â”œâ”€ P95: ${result.p95.toFixed(2)}ms
  â”œâ”€ P99: ${result.p99.toFixed(2)}ms
  â””â”€ Memory:
      â”œâ”€ Initial: ${this.formatMemory(result.memoryUsage.initial)}
      â”œâ”€ Peak: ${this.formatMemory(result.memoryUsage.peak)}
      â”œâ”€ Final: ${this.formatMemory(result.memoryUsage.final)}
      â””â”€ Leaked: ${this.formatMemory(result.memoryUsage.leaked)}
`)
  }
  
  /**
   * æ‰“å°æ€§èƒ½å¯¹æ¯”
   */
  private printComparison(): void {
    if (this.results.length === 0) return
    
    console.log('\nğŸ“Š Performance Comparison:')
    console.log('â•'.repeat(80))
    
    // æ‰¾å‡ºåŸºå‡†æµ‹è¯•
    const baseline = this.results.find(r => r.name === 'Baseline') || this.results[0]
    
    // æ‰“å°è¡¨å¤´
    console.log(
      '| Test Name'.padEnd(25) + 
      '| RPS'.padEnd(12) +
      '| Avg Time'.padEnd(12) +
      '| P99'.padEnd(10) +
      '| Memory'.padEnd(12) +
      '| vs Baseline |'
    )
    console.log('â”€'.repeat(80))
    
    // æ‰“å°æ¯ä¸ªç»“æœ
    for (const result of this.results) {
      const improvement = ((result.requestsPerSecond / baseline.requestsPerSecond) - 1) * 100
      const improvementStr = improvement >= 0 ? `+${improvement.toFixed(1)}%` : `${improvement.toFixed(1)}%`
      
      console.log(
        `| ${result.name.padEnd(23)} ` +
        `| ${result.requestsPerSecond.toFixed(1).padEnd(10)} ` +
        `| ${result.averageTime.toFixed(1).padEnd(10)} ` +
        `| ${result.p99.toFixed(1).padEnd(8)} ` +
        `| ${this.formatMemory(result.memoryUsage.peak).padEnd(10)} ` +
        `| ${improvementStr.padEnd(11)} |`
      )
    }
    
    console.log('â•'.repeat(80))
  }
  
  /**
   * è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆå­—èŠ‚ï¼‰
   */
  private getMemoryUsage(): number {
    if (typeof process !== 'undefined' && process.memoryUsage) {
      return process.memoryUsage().heapUsed
    }
    
    if (typeof performance !== 'undefined' && (performance as any).memory) {
      return (performance as any).memory.usedJSHeapSize
    }
    
    return 0
  }
  
  /**
   * æ ¼å¼åŒ–å†…å­˜å¤§å°
   */
  private formatMemory(bytes: number): string {
    if (bytes < 1024) return `${bytes}B`
    if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)}KB`
    return `${(bytes / 1024 / 1024).toFixed(1)}MB`
  }
  
  /**
   * å¼ºåˆ¶åƒåœ¾å›æ”¶
   */
  private forceGC(): void {
    if (typeof global !== 'undefined' && (global as any).gc) {
      try {
        (global as any).gc()
      } catch {
        // å¿½ç•¥é”™è¯¯
      }
    }
  }
  
  /**
   * è·å–ç™¾åˆ†ä½æ•°
   */
  private getPercentile(sortedArray: number[], percentile: number): number {
    if (sortedArray.length === 0) return 0
    const index = Math.ceil((percentile / 100) * sortedArray.length) - 1
    return sortedArray[Math.max(0, index)]
  }
  
  /**
   * ç¡çœ 
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }
}

/**
 * è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
 */
export async function runAllBenchmarks(): Promise<void> {
  const benchmark = new PerformanceBenchmark()
  
  console.log('ğŸ¯ Starting HTTP Client Performance Benchmarks\n')
  
  // 1. å†…å­˜æ³„æ¼æµ‹è¯•
  await benchmark.runMemoryLeakTest()
  console.log('\n')
  
  // 2. å‹åŠ›æµ‹è¯•
  await benchmark.runStressTest()
  
  console.log('\nâœ… All benchmarks completed!')
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶
if (require.main === module) {
  runAllBenchmarks().catch(console.error)
}